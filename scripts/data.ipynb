{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This notebook aims to clean and classify the image dataset for preparation to train the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in dataset = 7062\n"
     ]
    }
   ],
   "source": [
    "dataset = Path(\"data/India/\")\n",
    "num_datapoints = dataset.glob(pattern=\"*.jpg\")\n",
    "print(f\"Number of datapoints in dataset = {len(list(num_datapoints))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Experimentation with existing YOLO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_and_show_yolo_annots(filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads in an image from the YOLO dataset, creating and displaying bounding boxes on it.\n",
    "    \n",
    "    The bounding box information is taken from the accompanying YOLO annotation file.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(f\"./data/India/{filename}.jpg\")\n",
    "    dh, dw, _ = img.shape   \n",
    "\n",
    "    file_data = \"\"\n",
    "\n",
    "    with open(f\"./data/India/YOLO_Darknet/{filename}.txt\", 'r') as f:\n",
    "        file_data = f.readlines()\n",
    "\n",
    "    for data in file_data:\n",
    "        _, x, y, width, height = map(float, data.split(' '))\n",
    "\n",
    "        l, r = int((x - width / 2) * dw),   int((x + width / 2) * dw)\n",
    "        t, b = int((y - height /  2) * dh), int((y + height / 2) * dh)\n",
    "\n",
    "        l = max(0, l)\n",
    "        r = min(r, dw - 1)\n",
    "        t = max(0, t)\n",
    "        b = min(b, dh - 1)\n",
    "\n",
    "        cv2.rectangle(img, (l, t), (r, b), (255, 0, 0), 1)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 25, 5):\n",
    "    mark_and_show_yolo_annots(f\"SS21_13 {i:04}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from these, that the purpose of these annotations was to identify and mark out humans in the field of vision of the vehicle. While this has merit, and a wide range of applications can be based on these, it is not of particular interest to our study of obstacles and environment analysis as a whole on the road, but just a sub section of such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - VGG network for classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 17:44:52.086769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Let's import tensorflow to use the pre-existing VGG 19 model\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_vgg() -> tf.keras.models.Model:\n",
    "    # Define the layers that we specifically want.\n",
    "    # Content layer where we obtain our feature maps\n",
    "    content_layers = [\"block5_conv2\"]\n",
    "\n",
    "    # The styling layers we want\n",
    "    style_layers = [\n",
    "        \"block1_conv1\",\n",
    "        \"block2_conv1\",\n",
    "        \"block3_conv1\",\n",
    "        \"block4_conv1\",\n",
    "        \"block5_conv1\",\n",
    "    ]\n",
    "\n",
    "    # Store the model and make it's layers untrainable\n",
    "    vgg = tf.keras.applications.vgg19.VGG19(include_top=True, weights=\"imagenet\")\n",
    "    vgg.trainable = False\n",
    "\n",
    "    # Obtain style and content output layers and then merge them\n",
    "    style_layer_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "    content_layer_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "    all_model_outputs = style_layer_outputs + content_layer_outputs\n",
    "\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 17:45:05.372077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-03 17:45:05.630582: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2022-10-03 17:45:05.822643: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2022-10-03 17:45:05.916538: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 411041792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 17:45:07.907057: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 411041792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 0\n",
      "Non-trainable params: 143,667,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model = create_model_from_vgg()\n",
    "\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to create a dataset for training our network above.\n",
    "\n",
    "data_dir = Path(\"./data/India/\")\n",
    "\n",
    "# We get a generator to generate our images\n",
    "images = data_dir.glob(\"SS21_13 0[0-9][0-9][0-9].jpg\")\n",
    "out_dir = Path(\"./data/vgg_dataset/\")\n",
    "\n",
    "# Commented out so we don't accidentally run the operation again.\n",
    "# for img in images:\n",
    "#     shutil.copy(src=img, dst=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Now we can load the images previously, and process to a suitable dataset\n",
    "\n",
    "batch_size = 32\n",
    "height, width = 224, 224\n",
    "\n",
    "# Use TF to generate the dataset from the output directory.\n",
    "# The images are resized to 64*64 as that is the input of the model we created for training.\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    out_dir,\n",
    "    seed=420,\n",
    "    image_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    labels=None,\n",
    "    crop_to_aspect_ratio=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['']\n"
     ]
    }
   ],
   "source": [
    "# This is consistent since we did not feed any classes into the dataset (determining the classes is the aim)\n",
    "print(f\"Classes: {train_ds.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height=224, Width=224, Color channels=3\n"
     ]
    }
   ],
   "source": [
    "# We can see that we have batches of 32 images, each of which has been resized to 64x64 dims\n",
    "\n",
    "for image_batch in train_ds:\n",
    "    _, h, w, channles = image_batch.get_shape()\n",
    "    print(f\"Height={h}, Width={w}, Color channels={channles}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, before training and prediction, we just need to scale the RGB channel values of our images accordingly\n",
    "\n",
    "norm_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "norm_ds = train_ds.map(lambda x: (norm_layer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Model Fitting and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 256s 8s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = transfer_model.predict(x=norm_ds, batch_size=32, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('n03788365', 'mosquito_net', 0.09929915), ('n04209239', 'shower_curtain', 0.03335749), ('n03291819', 'envelope', 0.029101176)], [('n03788365', 'mosquito_net', 0.14031307), ('n04209239', 'shower_curtain', 0.044426326), ('n03291819', 'envelope', 0.029852847)], [('n03788365', 'mosquito_net', 0.11054545), ('n04209239', 'shower_curtain', 0.03444298), ('n03291819', 'envelope', 0.029767605)], [('n03788365', 'mosquito_net', 0.07276636), ('n03291819', 'envelope', 0.034083467), ('n04209239', 'shower_curtain', 0.03138446)], [('n03788365', 'mosquito_net', 0.107587725), ('n04209239', 'shower_curtain', 0.03427578), ('n03291819', 'envelope', 0.024215296)], [('n03788365', 'mosquito_net', 0.08296403), ('n04209239', 'shower_curtain', 0.032409992), ('n03291819', 'envelope', 0.031521685)], [('n03788365', 'mosquito_net', 0.17276318), ('n04209239', 'shower_curtain', 0.03240079), ('n03291819', 'envelope', 0.02808046)], [('n03788365', 'mosquito_net', 0.064780906), ('n04209239', 'shower_curtain', 0.032214075), ('n03291819', 'envelope', 0.03085653)], [('n03788365', 'mosquito_net', 0.089231096), ('n04209239', 'shower_curtain', 0.037162893), ('n03291819', 'envelope', 0.029082507)], [('n03788365', 'mosquito_net', 0.076237686), ('n04209239', 'shower_curtain', 0.032001216), ('n03291819', 'envelope', 0.029055981)]]\n"
     ]
    }
   ],
   "source": [
    "labels = tf.keras.applications.vgg19.decode_predictions(y_pred, top=3)\n",
    "\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we can see that the top 3 predictions for our dataset by the VGG19 based model are \n",
    "1. mosquito nets\n",
    "2. envelopes\n",
    "3. shower curtains\n",
    "\n",
    "This is incorrect, as we know from knowledge of our dataset, and hence we can conclude that using features extracted from a pre-trained model is not very effective here since the problem space of the model is too general."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52685b9acab1763b7d02feee1bddc1680804e9db5ebbd2dd4325b19f88cf9b68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
